# UK Train Delays ETL Pipeline

## Project Overview
This project is an ETL pipeline for UK train delays data. It extracts data from CSV files, transforms it, and loads it into a PostgreSQL database.
## Project Structure
The project is organized into the following files:
- `etl.py`: Contains the ETL logic for extracting, transforming, and loading data.
- `README.md`: This file, which provides an overview of the project and set up instructions.
- `requirements.txt`: Lists the required Python packages for the project.
- `data/`: Contains the input CSV files used for the ETL process.
 - `data/raw/`: Folder containing the raw data CSV files.
- `data/output/`: Contains the output data files generated by the ETL process.

## Project Layout
- uk-train-delays/: Root directory of the project.
- dags/: Contains the Airflow DAGs definitions.
 - train_delays_dag.py: Main ETL pipeline DAG.
- scripts/: Contains the ETL script.
 - extract_data.py: Extracts data from CSV files.
 - transform_data.py: Cleans and processes the extracted data.
 - load_data.py: Loads the transformed data into a PostgreSQL database.
- data/: Stores data in raw and processed formats.
 - raw/: Contains the raw data CSV files.
 - processed/: Contains the processed data files.
- diagrams/: Contains diagrams and visualizations related to the project.
 - data_model_png: Data flow diagram.
- Airflow/: Contains the Airflow DAGs definitions.
 - docker-compose.yml: Docker Compose configuration for running Airflow in Docker.
- notebooks/: Contains Jupyter notebooks for data exploration and analysis.
- README.md: Project documentation.
- requirements.txt: Lists the required Python packages for the project.
- .env: Environment variables file for configuration.
