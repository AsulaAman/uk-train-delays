# UK Train Delays ETL Pipeline

## Project Overview
This project is an ETL pipeline for UK train delays data. It extracts data from CSV files, transforms it, and loads it into a PostgreSQL database.
## Project Structure
The project is organized into the following files:
- `README.md`: This file provides an overview of the project and setup instructions.
- `requirements.txt`: Lists the required Python packages for the project.
- `dags`: Contains the ETL logic for extracting, transforming, and loading data.
- `scripts`: Contains the ETL script for extracting, transforming, and loading data.
- `data/`: Contains the input CSV files used for the ETL process.
  - `data/raw/`: Folder containing the raw data CSV files.
  - `data/processed/`: Contains the output data files generated by the ETL process.
- `airflow/`: Contains Docker setup, logins and plugins.
  - `docker-compose.yml`: Docker Compose configuration for running Airflow in Docker.
  - `logs`: Logs generated by Airflow.
  - `plugins`: Custom Airflow plugins.
- `airflow.cfg`: Configuration file for Airflow.
- `webserver_config.py`: Configuration file for the Airflow webserver.

## Project Layout
- `uk-train-delays/`: Root directory of the project.

- `dags/`: Contains the Airflow DAGs definitions.
 - `train_delays_dag.py`: Main ETL pipeline DAG.
 - `cancellations_etl_dag.py`: ETL for cancellations data.
 - `punctuality_etl_dag.py`: ETL for punctuality data.


- `scripts/`: Contains the ETL scripts.
 - `extract_data.py`: Extracts data from CSV files.
 - `transform_data.py`: Cleans and processes the extracted data.
 - `load_punctuality.py`, `load_to_db` & `load_train_delays_to_db`: Loads various datasets into PostgreSQL.
 - `check_columns.py`: Checks the columns of the data.
